{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\swapnil jain\\desktop\\audio_classification\\audio_classification\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\swapnil jain\\desktop\\audio_classification\\audio_classification\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\swapnil jain\\desktop\\audio_classification\\audio_classification\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\swapnil jain\\desktop\\audio_classification\\audio_classification\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\swapnil jain\\desktop\\audio_classification\\audio_classification\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\swapnil jain\\desktop\\audio_classification\\audio_classification\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\swapnil jain\\desktop\\audio_classification\\audio_classification\\.venv\\lib\\site-packages (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\SWAPNIL JAIN\\\\Desktop\\\\Audio_classification\\\\Audio_Classification'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SWAPNIL JAIN\\Desktop\\Audio_classification\\Audio_Classification\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\SWAPNIL JAIN\\Desktop\\Audio_classification\\Audio_Classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 99ms/step - accuracy: 0.0622 - loss: 0.3997 - val_accuracy: 0.2270 - val_loss: 0.2689\n",
      "Epoch 2/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 92ms/step - accuracy: 0.2349 - loss: 0.2639 - val_accuracy: 0.2860 - val_loss: 0.2497\n",
      "Epoch 3/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 113ms/step - accuracy: 0.2963 - loss: 0.2440 - val_accuracy: 0.3130 - val_loss: 0.2434\n",
      "Epoch 4/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 103ms/step - accuracy: 0.3192 - loss: 0.2337 - val_accuracy: 0.3190 - val_loss: 0.2339\n",
      "Epoch 5/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 94ms/step - accuracy: 0.3285 - loss: 0.2240 - val_accuracy: 0.3290 - val_loss: 0.2337\n",
      "Epoch 6/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 92ms/step - accuracy: 0.3337 - loss: 0.2168 - val_accuracy: 0.3190 - val_loss: 0.2306\n",
      "Epoch 7/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 111ms/step - accuracy: 0.3569 - loss: 0.2096 - val_accuracy: 0.3220 - val_loss: 0.2307\n",
      "Epoch 8/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.3566 - loss: 0.2058 - val_accuracy: 0.3090 - val_loss: 0.2292\n",
      "Epoch 9/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 96ms/step - accuracy: 0.3583 - loss: 0.1988 - val_accuracy: 0.3120 - val_loss: 0.2288\n",
      "Epoch 10/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 94ms/step - accuracy: 0.3566 - loss: 0.1919 - val_accuracy: 0.3250 - val_loss: 0.2302\n",
      "Epoch 11/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 92ms/step - accuracy: 0.3741 - loss: 0.1872 - val_accuracy: 0.3150 - val_loss: 0.2317\n",
      "Epoch 12/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 90ms/step - accuracy: 0.3641 - loss: 0.1831 - val_accuracy: 0.3180 - val_loss: 0.2323\n",
      "Epoch 13/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 91ms/step - accuracy: 0.3694 - loss: 0.1766 - val_accuracy: 0.3140 - val_loss: 0.2359\n",
      "Epoch 14/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 94ms/step - accuracy: 0.3745 - loss: 0.1706 - val_accuracy: 0.3270 - val_loss: 0.2326\n",
      "Epoch 15/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 92ms/step - accuracy: 0.3837 - loss: 0.1632 - val_accuracy: 0.3320 - val_loss: 0.2368\n",
      "Epoch 16/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 91ms/step - accuracy: 0.3858 - loss: 0.1579 - val_accuracy: 0.3290 - val_loss: 0.2402\n",
      "Epoch 17/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 94ms/step - accuracy: 0.3870 - loss: 0.1560 - val_accuracy: 0.3160 - val_loss: 0.2488\n",
      "Epoch 18/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 93ms/step - accuracy: 0.4171 - loss: 0.1473 - val_accuracy: 0.3310 - val_loss: 0.2509\n",
      "Epoch 19/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 93ms/step - accuracy: 0.3938 - loss: 0.1412 - val_accuracy: 0.3180 - val_loss: 0.2548\n",
      "Epoch 20/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 94ms/step - accuracy: 0.3983 - loss: 0.1366 - val_accuracy: 0.3270 - val_loss: 0.2709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/06 02:18:29 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2025/02/06 02:18:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run flawless-auk-142 at: http://127.0.0.1:5000/#/experiments/0/runs/73e8c66c1eb14780b4963d37073db96e\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/0\n",
      "Model successfully logged in MLflow!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Example prediction\u001b[39;00m\n\u001b[0;32m    106\u001b[0m audio_sample_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martifacts/data_preprocessing/processed_audio_data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmixed_5.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m    107\u001b[0m                       \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martifacts/data_preprocessing/processed_audio_data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmixed_10.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 108\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_and_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_sample_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_labels)\n",
      "Cell \u001b[1;32mIn[15], line 101\u001b[0m, in \u001b[0;36mpredict_and_decode\u001b[1;34m(model, audio_paths, mlb, threshold)\u001b[0m\n\u001b[0;32m     98\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(features)\n\u001b[0;32m    100\u001b[0m binary_predictions \u001b[38;5;241m=\u001b[39m (predictions \u001b[38;5;241m>\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m--> 101\u001b[0m decoded_predictions \u001b[38;5;241m=\u001b[39m [\u001b[43mmlb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m binary_predictions]\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_predictions\n",
      "File \u001b[1;32mc:\\Users\\SWAPNIL JAIN\\Desktop\\Audio_classification\\Audio_Classification\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:934\u001b[0m, in \u001b[0;36mMultiLabelBinarizer.inverse_transform\u001b[1;34m(self, yt)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform the given indicator matrix into label sets.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \n\u001b[0;32m    921\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;124;03m    `classes_[j]` for each `yt[i, j] == 1`.\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    932\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43myt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_):\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected indicator for \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m classes, but got \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    937\u001b[0m             \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_), yt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    938\u001b[0m         )\n\u001b[0;32m    939\u001b[0m     )\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(yt):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import librosa\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the MLflow tracking URI to the local server\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "\n",
    "# Load VGGish model\n",
    "vggish = hub.load(\"https://tfhub.dev/google/vggish/1\")\n",
    "\n",
    "# Function to extract features from audio files\n",
    "def extract_vggish_features(audio_path):\n",
    "    audio, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "    audio_tensor = tf.convert_to_tensor(audio, dtype=tf.float32)\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    embeddings = vggish(audio_tensor)\n",
    "    return tf.keras.backend.eval(embeddings)\n",
    "\n",
    "# Load dataset from a CSV file\n",
    "def load_dataset(csv_file, label_map):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    audio_paths = df[\"file_path\"].tolist()\n",
    "\n",
    "    # Convert string labels to actual list of labels\n",
    "    raw_labels = [eval(label_str) for label_str in df[\"labels_text\"]]\n",
    "\n",
    "    # Convert labels to binary format (multi-hot encoding)\n",
    "    mlb = MultiLabelBinarizer(classes=list(label_map.keys()))\n",
    "    encoded_labels = mlb.fit_transform(raw_labels)\n",
    "\n",
    "    return audio_paths, encoded_labels, mlb\n",
    "\n",
    "# Label mapping\n",
    "label_map = {\n",
    "    'Fire': 1, 'Rain': 2, 'Thunderstorm': 3, 'WaterDrops': 4, 'Wind': 5, 'Silence': 6, 'TreeFalling': 7, \"Helicopter\": 8,\n",
    "   \"VehicleEngine\":9, \"Axe\":10, \"Chainsaw\":11, \"Generator\":12, \"Handsaw\":13,  \"Firework\":14, \"Gunshot\":15,  \"WoodChop\":16,\n",
    "   \"Whistling\":17,\"Speaking\":18,\"Footsteps\":19,\"Clapping\":20, \"Insect\":21, \"Frog\":22,\"BirdChapping\":23,\"WingFlapping\":24,\n",
    "   \"Lion\":25, \"WolfHowl\":26, \"Squirrel\":27\n",
    "}\n",
    "\n",
    "# Load dataset\n",
    "csv_file_path = r\"C:\\Users\\SWAPNIL JAIN\\Desktop\\Audio_classification\\Audio_Classification\\preprocessed_data.csv\"\n",
    "audio_paths, encoded_labels, mlb = load_dataset(csv_file_path, label_map)\n",
    "\n",
    "# Extract features\n",
    "features = np.stack([extract_vggish_features(path) for path in audio_paths], axis=0)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(5, 128)),  \n",
    "    tf.keras.layers.LSTM(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(len(label_map), activation=\"sigmoid\")  \n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model with MLflow logging\n",
    "with mlflow.start_run():\n",
    "       \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "    mlflow.tensorflow.log_model(model, \"model\")\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"epochs\": 20,\n",
    "        \"batch_size\": 32\n",
    "    })\n",
    "\n",
    "    # Log metrics\n",
    "    for epoch in range(20):\n",
    "        mlflow.log_metric(\"train_loss\", history.history[\"loss\"][epoch], step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", history.history[\"val_loss\"][epoch], step=epoch)\n",
    "        mlflow.log_metric(\"train_accuracy\", history.history[\"accuracy\"][epoch], step=epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", history.history[\"val_accuracy\"][epoch], step=epoch)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Save as a pickle file and log as artifact\n",
    "    joblib.dump(model, 'model.pkl')\n",
    "    mlflow.log_artifact('model.pkl')\n",
    "\n",
    "print(\"Model successfully logged in MLflow!\")\n",
    "\n",
    "# Prediction function\n",
    "def predict_and_decode(model, audio_paths, mlb, threshold=0.5):\n",
    "    features = np.array([extract_vggish_features(path) for path in audio_paths])\n",
    "    predictions = model.predict(features)\n",
    "    \n",
    "    binary_predictions = (predictions > threshold).astype(int)\n",
    "    decoded_predictions = [mlb.inverse_transform([prediction])[0] for prediction in binary_predictions]\n",
    "    \n",
    "    return decoded_predictions\n",
    "\n",
    "# Example prediction\n",
    "audio_sample_paths = [r\"artifacts/data_preprocessing/processed_audio_data\\mixed_5.wav\", \n",
    "                      r\"artifacts/data_preprocessing/processed_audio_data\\mixed_10.wav\"]\n",
    "predicted_labels = predict_and_decode(model, audio_sample_paths, mlb)\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement json (from versions: none)\n",
      "ERROR: No matching distribution found for json\n"
     ]
    }
   ],
   "source": [
    "pip install json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SWAPNIL JAIN\\Desktop\\Audio_classification\\Audio_Classification\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step\n",
      "{\"predictions\": [[\"Helicopter\", \"Insect\"], [\"Firework\", \"Speaking\"]]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Corrected Prediction Function with JSON Formatting\n",
    "def predict_and_decode(model, audio_paths, mlb, threshold=0.5):\n",
    "    features = [extract_vggish_features(path) for path in audio_paths]  # List of (5, 128) arrays\n",
    "    \n",
    "    # Ensure features are stacked properly\n",
    "    features = np.stack(features, axis=0)  # Shape: (num_samples, 5, 128)\n",
    "    \n",
    "    # Ensure batch dimension is correctly formatted\n",
    "    predictions = model.predict(features)  # Shape: (num_samples, num_classes)\n",
    "    \n",
    "    # Convert probabilities to binary labels using threshold\n",
    "    binary_predictions = (predictions > threshold).astype(int)  # Shape: (num_samples, num_classes)\n",
    "    \n",
    "    # Decode multi-hot encoded predictions into human-readable labels\n",
    "    decoded_predictions = mlb.inverse_transform(binary_predictions)  # Returns list of label tuples\n",
    "\n",
    "    # Convert the predictions into a JSON-compatible structure\n",
    "    json_predictions = json.dumps({\"predictions\": [list(labels) for labels in decoded_predictions]})\n",
    "    \n",
    "    return json_predictions  # Return predictions in JSON format\n",
    "\n",
    "# Example prediction\n",
    "audio_sample_paths = [\n",
    "    r\"artifacts/data_preprocessing/processed_audio_data/mixed_5.wav\",\n",
    "    r\"artifacts/data_preprocessing/processed_audio_data/mixed_10.wav\"\n",
    "]\n",
    "\n",
    "predicted_labels_json = predict_and_decode(model, audio_sample_paths, mlb)\n",
    "print(predicted_labels_json)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
